import requests
from bs4 import BeautifulSoup
import time
import random
import re
import os
import django

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings')
django.setup()

from main_app.models import LianJiaHouse


class LianJiaBypassSpider:
    def __init__(self):
        self.base_url = "https://bj.lianjia.com/ershoufang/"
        self.session = requests.Session()

        # è®¾ç½®éå¸¸çœŸå®çš„è¯·æ±‚å¤´
        self.session.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://bj.lianjia.com/',
        }

    def get_districts(self):
        return {
            'chaoyang': 'æœé˜³',
            'haidian': 'æµ·æ·€',
            'dongcheng': 'ä¸œåŸ',
            'xicheng': 'è¥¿åŸ'
        }

    def get_page_with_proxy(self, url, max_retries=3):
        """ä½¿ç”¨ä»£ç†å’Œé‡è¯•æœºåˆ¶è·å–é¡µé¢"""
        for attempt in range(max_retries):
            try:
                # æ¯æ¬¡è¯·æ±‚å‰éšæœºå»¶è¿Ÿ
                delay = random.uniform(5, 15)
                print(f"â° å»¶è¿Ÿ {delay:.1f}ç§’åè¯·æ±‚...")
                time.sleep(delay)

                # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼šå…ˆè®¿é—®é¦–é¡µ
                if attempt == 0:
                    self.session.get("https://bj.lianjia.com/", timeout=10)
                    time.sleep(2)

                response = self.session.get(url, timeout=15)

                if response.status_code == 200:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸå®é¡µé¢
                    if "sellListContent" in response.text:
                        print("âœ… æˆåŠŸè·å–æˆ¿æºé¡µé¢")
                        return response
                    elif "éªŒè¯" in response.text or "login" in response.text:
                        print(f"ğŸš« ç¬¬{attempt + 1}æ¬¡å°è¯•è¢«åçˆ¬")
                        continue
                    else:
                        print("âš ï¸ é¡µé¢ç»“æ„å¼‚å¸¸")
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")

            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

            # é‡è¯•å‰ç­‰å¾…æ›´é•¿æ—¶é—´
            time.sleep(10 * (attempt + 1))

        return None

    def parse_page_smart(self, html, district):
        """æ™ºèƒ½è§£æé¡µé¢ï¼Œè‡ªåŠ¨é€‚åº”ä¸åŒç»“æ„"""
        soup = BeautifulSoup(html, 'html.parser')
        houses = []

        # å¤šç§å¯èƒ½çš„æˆ¿æºå®¹å™¨é€‰æ‹©å™¨
        container_selectors = [
            '.sellListContent',
            '.content__list',
            '.house-lst',
            '.ershoufang-list',
            '[class*="list-content"]'
        ]

        # å¤šç§æˆ¿æºé¡¹é€‰æ‹©å™¨
        item_selectors = [
            'li',
            '.item',
            '.list-item',
            '[class*="info"]'
        ]

        for container_selector in container_selectors:
            container = soup.select_one(container_selector)
            if container:
                print(f"âœ… æ‰¾åˆ°å®¹å™¨: {container_selector}")

                for item_selector in item_selectors:
                    items = container.select(item_selector)
                    if items:
                        print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨: {container_selector} {item_selector}, æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")

                        for item in items:
                            house_data = self.parse_house_item_smart(item, district)
                            if house_data:
                                houses.append(house_data)
                        break
                break

        if not houses:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æˆ¿æºæ•°æ®")
            # ä¿å­˜é¡µé¢ç”¨äºåˆ†æ
            with open(f"analysis_{district}.html", "w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ’¾ åˆ†æé¡µé¢å·²ä¿å­˜: analysis_{district}.html")

        return houses

    def parse_house_item_smart(self, item, district):
        """æ™ºèƒ½è§£ææˆ¿æºé¡¹"""
        try:
            # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
            title = None
            detail_url = None

            # æŸ¥æ‰¾åŒ…å«"ershoufang"çš„é“¾æ¥
            links = item.find_all('a', href=re.compile(r'ershoufang'))
            for link in links:
                if link.text.strip():
                    title = link.text.strip()
                    detail_url = link.get('href', '')
                    if detail_url and not detail_url.startswith('http'):
                        detail_url = 'https://bj.lianjia.com' + detail_url
                    break

            if not title:
                return None

            # æŸ¥æ‰¾ä»·æ ¼ä¿¡æ¯
            price_text = item.get_text()
            total_price = 0
            price_match = re.search(r'(\d+\.?\d*)\s*ä¸‡', price_text)
            if price_match:
                total_price = float(price_match.group(1))

            # æŸ¥æ‰¾å•ä»·
            unit_price = 0
            unit_match = re.search(r'å•ä»·(\d+)', price_text)
            if unit_match:
                unit_price = float(unit_match.group(1))

            # åŒºåŸŸ
            districts = self.get_districts()
            district_cn = districts.get(district, district)

            return {
                'title': title,
                'total_price': total_price,
                'unit_price': unit_price,
                'district': district_cn,
                'area': 0,  # ç®€åŒ–å¤„ç†
                'layout': 'æœªçŸ¥',
                'xiaoqu': 'æœªçŸ¥',
                'floor': 'æœªçŸ¥',
                'orientation': 'æœªçŸ¥',
                'description': title,
                'source_url': detail_url,
                'city': 'åŒ—äº¬',
            }

        except Exception as e:
            print(f"è§£ææˆ¿æºå¤±è´¥: {e}")
            return None

    def save_houses(self, houses):
        """ä¿å­˜æ•°æ®"""
        saved_count = 0
        for house_data in houses:
            try:
                existing = LianJiaHouse.objects.filter(
                    title=house_data['title'],
                    source_url=house_data['source_url']
                ).exists()

                if not existing:
                    LianJiaHouse.objects.create(**house_data)
                    saved_count += 1
                    print(f"âœ… ä¿å­˜: {house_data['title'][:30]}... - {house_data['total_price']}ä¸‡")
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

        return saved_count

    def start_crawl(self, max_pages_per_district=2):
        """å¼€å§‹çˆ¬è™«"""
        print("ğŸš€ å¼€å§‹ç»•è¿‡åçˆ¬çˆ¬å–é“¾å®¶æ•°æ®...")

        districts = self.get_districts()
        total_saved = 0

        for district_en, district_cn in districts.items():
            print(f"\nğŸ“ å¼€å§‹çˆ¬å– {district_cn} åŒºåŸŸ...")

            for page in range(1, max_pages_per_district + 1):
                url = f"{self.base_url}{district_en}/pg{page}/"
                print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

                response = self.get_page_with_proxy(url)
                if not response:
                    print(f"âŒ è·³è¿‡ {district_cn} ç¬¬ {page} é¡µ")
                    continue

                houses = self.parse_page_smart(response.text, district_en)
                saved_count = self.save_houses(houses)
                total_saved += saved_count

                print(f"ğŸ“Š {district_cn} ç¬¬ {page} é¡µ: è·å– {len(houses)} æ¡ï¼Œæ–°å¢ {saved_count} æ¡")

        print(f"\nğŸ‰ ç»•è¿‡çˆ¬å–å®Œæˆï¼æ–°å¢ {total_saved} æ¡æ•°æ®")
        return total_saved