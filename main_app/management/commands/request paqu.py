import requests
import csv
import time
from bs4 import BeautifulSoup
import random
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()


def multi_city_crawler_safe():
    """å®‰å…¨çš„å¤šåŸå¸‚çˆ¬è™« - æ¯ä¸ªåŸå¸‚çˆ¬5é¡µï¼Œé¿å…è§¦å‘åçˆ¬"""

    cities = [
        ('bj', 'åŒ—äº¬'),
        ('sh', 'ä¸Šæµ·'),
        ('gz', 'å¹¿å·'),
        ('sz', 'æ·±åœ³'),
        ('hz', 'æ­å·'),
        ('nj', 'å—äº¬'),
        ('wh', 'æ­¦æ±‰'),
        ('cd', 'æˆéƒ½')
    ]

    all_houses = []

    for city_code, city_name in cities:
        logger.info(f"\nğŸš€ å¼€å§‹çˆ¬å– {city_name} æ•°æ®...")

        city_houses = crawl_city_safe(city_code, city_name, max_pages=5)
        all_houses.extend(city_houses)

        logger.info(f"âœ… {city_name} å®Œæˆï¼Œè·å– {len(city_houses)} æ¡æ•°æ®")

        # åŸå¸‚é—´è¾ƒé•¿å»¶è¿Ÿï¼Œé¿å…è¢«å°
        delay = random.uniform(10, 20)
        logger.info(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåŸå¸‚...")
        time.sleep(delay)

    # ä¿å­˜æ‰€æœ‰æ•°æ®
    filename = f'multi_city_houses_{len(all_houses)}.csv'
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(
            ['åŸå¸‚', 'æ ‡é¢˜', 'æ€»ä»·(ä¸‡)', 'å•ä»·(å…ƒ/å¹³)', 'æˆ·å‹', 'é¢ç§¯(å¹³)', 'æœå‘', 'è£…ä¿®', 'æ¥¼å±‚', 'å¹´ä»½', 'å°åŒº',
             'åŒºåŸŸ', 'é“¾æ¥'])

        for house in all_houses:
            writer.writerow([
                house['city'],
                house['title'],
                house['total_price'],
                house['unit_price'],
                house['layout'],
                house['area'],
                house['direction'],
                house['decoration'],
                house['floor'],
                house['year'],
                house['community'],
                house['district'],
                house['link']
            ])

    logger.info(f"\nğŸ‰ æ‰€æœ‰åŸå¸‚çˆ¬å–å®Œæˆï¼æ€»å…±è·å– {len(all_houses)} æ¡æ•°æ®")
    logger.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {filename}")

    return len(all_houses)


def crawl_city_safe(city_code, city_name, max_pages=5):
    """å®‰å…¨çˆ¬å–å•ä¸ªåŸå¸‚æ•°æ®"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    }

    session = requests.Session()
    session.headers.update(headers)

    city_houses = []

    for page in range(1, max_pages + 1):
        logger.info(f"  æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

        # æ„é€ URL
        if page == 1:
            url = f'https://{city_code}.lianjia.com/ershoufang/'
        else:
            url = f'https://{city_code}.lianjia.com/ershoufang/pg{page}/'

        try:
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(2, 4))

            response = session.get(url, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                # æ£€æŸ¥æ˜¯å¦è¢«é™åˆ¶
                if "æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æˆ¿æº" in response.text or "è®¿é—®éªŒè¯" in response.text:
                    logger.warning(f"  âš ï¸  {city_name} ç¬¬ {page} é¡µè¢«é™åˆ¶ï¼Œè·³è¿‡")
                    break

                house_list = soup.select('.sellListContent li')
                logger.info(f"  ç¬¬ {page} é¡µæ‰¾åˆ° {len(house_list)} ä¸ªæˆ¿å±‹")

                if len(house_list) == 0:
                    logger.warning(f"  âš ï¸  {city_name} ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢")
                    break

                page_count = 0
                for house in house_list:
                    house_data = extract_house_safe(house)
                    if house_data:
                        house_data['city'] = city_name
                        city_houses.append(house_data)
                        page_count += 1

                logger.info(f"  âœ… ç¬¬{page}é¡µæå– {page_count} ä¸ªæˆ¿å±‹")

            else:
                logger.warning(f"  âŒ {city_name} ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
                break

        except Exception as e:
            logger.error(f"  âŒ {city_name} ç¬¬ {page} é¡µå‡ºé”™: {e}")
            break

    return city_houses


def extract_house_safe(house_element):
    """å®‰å…¨æå–æˆ¿å±‹æ•°æ®"""
    try:
        data = {}

        # æ ‡é¢˜å’Œé“¾æ¥
        title_elem = house_element.select_one('.title a')
        if not title_elem:
            return None
        data['title'] = title_elem.text.strip()
        data['link'] = title_elem.get('href', '')

        # ä»·æ ¼
        total_price_elem = house_element.select_one('.totalPrice span')
        data['total_price'] = total_price_elem.text.strip() if total_price_elem else ""

        unit_price_elem = house_element.select_one('.unitPrice')
        data['unit_price'] = unit_price_elem.text.strip() if unit_price_elem else ""

        # æˆ¿å±‹ä¿¡æ¯
        house_info_elem = house_element.select_one('.houseInfo')
        if house_info_elem:
            info_parts = house_info_elem.text.strip().split('|')
            data['layout'] = info_parts[0].strip() if len(info_parts) > 0 else ""
            data['area'] = info_parts[1].strip() if len(info_parts) > 1 else ""
            data['direction'] = info_parts[2].strip() if len(info_parts) > 2 else ""
            data['decoration'] = info_parts[3].strip() if len(info_parts) > 3 else ""
            data['floor'] = info_parts[4].strip() if len(info_parts) > 4 else ""
            data['year'] = info_parts[5].strip() if len(info_parts) > 5 else ""

        # ä½ç½®ä¿¡æ¯
        position_elem = house_element.select_one('.positionInfo')
        if position_elem:
            position_info = position_elem.text.strip()
            position_parts = position_info.split('-')
            data['community'] = position_parts[0].strip() if len(position_parts) > 0 else ""
            data['district'] = position_parts[1].strip() if len(position_parts) > 1 else ""

        return data

    except Exception:
        return None


# è¿è¡Œå¤šåŸå¸‚çˆ¬è™«
if __name__ == "__main__":
    total_count = multi_city_crawler_safe()
    print(f"\næœ€ç»ˆç»“æœ: æˆåŠŸçˆ¬å– {total_count} æ¡æˆ¿å±‹æ•°æ®ï¼")