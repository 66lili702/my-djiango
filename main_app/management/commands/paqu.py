from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
import time
import csv


def final_lianjia_crawler(city='bj', max_pages=5):
    """æœ€ç»ˆç‰ˆé“¾å®¶çˆ¬è™« - ä¸€é”®è¿è¡Œ"""

    print(f"ğŸš€ å¼€å§‹çˆ¬å– {city} äºŒæ‰‹æˆ¿æ•°æ®ï¼Œæœ€å¤š {max_pages} é¡µ")

    # æµè§ˆå™¨è®¾ç½®
    chrome_options = Options()
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')

    service = ChromeService(r'D:\Users\lenovo\PycharmProjects\djiangoæ¯•è®¾\chromedriver.exe')
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        filename = f'final_{city}_houses.csv'
        with open(filename, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(
                ['æ ‡é¢˜', 'æ€»ä»·(ä¸‡)', 'å•ä»·(å…ƒ/å¹³)', 'æˆ·å‹', 'é¢ç§¯(å¹³)', 'æœå‘', 'è£…ä¿®', 'æ¥¼å±‚', 'å¹´ä»½', 'å°åŒº', 'åŒºåŸŸ',
                 'é“¾æ¥'])

            total_houses = 0

            for page in range(1, max_pages + 1):
                print(f"\nğŸ“„ ç¬¬ {page}/{max_pages} é¡µ...")

                # è®¿é—®é¡µé¢
                if page == 1:
                    url = f'https://{city}.lianjia.com/ershoufang/'
                else:
                    url = f'https://{city}.lianjia.com/ershoufang/pg{page}/'

                driver.get(url)
                time.sleep(3)

                # è·å–æˆ¿å±‹åˆ—è¡¨
                houses = driver.find_elements(By.CSS_SELECTOR, '.sellListContent li')
                print(f"æ‰¾åˆ° {len(houses)} ä¸ªæˆ¿å±‹")

                if len(houses) == 0:
                    print("âš ï¸  æ²¡æœ‰æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break

                page_count = 0
                for house in houses:
                    house_data = safe_extract(house)
                    if house_data and house_data['title']:  # ç¡®ä¿æœ‰æ ‡é¢˜
                        writer.writerow([
                            house_data['title'],
                            house_data.get('total_price', ''),
                            house_data.get('unit_price', ''),
                            house_data.get('layout', ''),
                            house_data.get('area', ''),
                            house_data.get('direction', ''),
                            house_data.get('decoration', ''),
                            house_data.get('floor', ''),
                            house_data.get('year', ''),
                            house_data.get('community', ''),
                            house_data.get('district', ''),
                            house_data.get('link', '')
                        ])
                        page_count += 1
                        total_houses += 1

                print(f"âœ… ç¬¬{page}é¡µæå– {page_count} ä¸ªæˆ¿å±‹")

                if page_count == 0:
                    print("âš ï¸  æœ¬é¡µæ²¡æœ‰æå–åˆ°æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break

                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ

        print(f"\nğŸ‰ å®Œæˆï¼å…±æå– {total_houses} ä¸ªæˆ¿å±‹")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åˆ°: {filename}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        driver.quit()


def safe_extract(house_element):
    """å®‰å…¨æå–æ•°æ®"""
    data = {}

    # æ ‡é¢˜å’Œé“¾æ¥
    try:
        title_elem = house_element.find_element(By.CSS_SELECTOR, '.title a')
        data['title'] = title_elem.text
        data['link'] = title_elem.get_attribute('href')
    except:
        return None

    # ä»·æ ¼
    try:
        price_elem = house_element.find_element(By.CSS_SELECTOR, '.totalPrice span')
        data['total_price'] = price_elem.text
    except:
        data['total_price'] = ''

    # å•ä»·
    try:
        unit_price_elem = house_element.find_element(By.CSS_SELECTOR, '.unitPrice')
        data['unit_price'] = unit_price_elem.text
    except:
        data['unit_price'] = ''

    # æˆ¿å±‹ä¿¡æ¯
    try:
        info_elem = house_element.find_element(By.CSS_SELECTOR, '.houseInfo')
        info_parts = info_elem.text.split('|')
        data['layout'] = info_parts[0].strip() if len(info_parts) > 0 else ""
        data['area'] = info_parts[1].strip() if len(info_parts) > 1 else ""
        data['direction'] = info_parts[2].strip() if len(info_parts) > 2 else ""
        data['decoration'] = info_parts[3].strip() if len(info_parts) > 3 else ""
        data['floor'] = info_parts[4].strip() if len(info_parts) > 4 else ""
        data['year'] = info_parts[5].strip() if len(info_parts) > 5 else ""
    except:
        pass

    # ä½ç½®ä¿¡æ¯
    try:
        pos_elem = house_element.find_element(By.CSS_SELECTOR, '.positionInfo')
        pos_parts = pos_elem.text.split('-')
        data['community'] = pos_parts[0].strip() if len(pos_parts) > 0 else ""
        data['district'] = pos_parts[1].strip() if len(pos_parts) > 1 else ""
    except:
        pass

    return data


# ğŸ¯ ä¸€é”®è¿è¡Œï¼
if __name__ == "__main__":
    # ä½ å¯ä»¥ä¿®æ”¹è¿™äº›å‚æ•°
    city = 'bj'  # åŸå¸‚: bj-åŒ—äº¬, sh-ä¸Šæµ·, gz-å¹¿å·, sz-æ·±åœ³
    pages = 3  # çˆ¬å–é¡µæ•°

    final_lianjia_crawler(city=city, max_pages=pages)